{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# Training a BERT model from scratch\n",
    "\n",
    "This demo illustrates the training of a BERT model from scratch. The model is trained on a small dataset of wolfSSL, which is an SSL library used in embedded software development. \n",
    "\n",
    "The notebook is based on the existing tutorial from Hugging Face [link](https://huggingface.co/blog/how-to-train).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aleXCJqVQPro"
   },
   "source": [
    "# Training a tokenizer\n",
    "\n",
    "The first step in training the model is to train the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMnymRDLe0hi",
    "outputId": "b7780f27-ce3b-4a3f-fcf5-ba11529b1d44",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files\n",
      "First file: source_code_wolf_ssl.txt\n"
     ]
    }
   ],
   "source": [
    "# We use the standard BPE tokenizer for this workbook\n",
    "# it was described in the previous chapter of the book\n",
    "# when we discussed feature extraction\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = ['source_code_wolf_ssl.txt']\n",
    "\n",
    "print(f'Found {len(paths)} files')\n",
    "print(f'First file: {paths[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "print('Training tokenizer...')\n",
    "\n",
    "# Customize training\n",
    "# we use a large vocabulary size, but we could also do with ca. 10_000\n",
    "tokenizer.train(files=paths, \n",
    "                vocab_size=52_000, \n",
    "                min_frequency=2, \n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqYKX1XYyRI-",
    "outputId": "8e2f0316-455d-443b-ca8b-146bdd2e3a6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolfBERTa/vocab.json', 'wolfBERTa/merges.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# we give this model a catchy name - wolfBERTa\n",
    "# because it is a RoBERTa model trained on the WolfSSL source code\n",
    "token_dir = './wolfBERTa'\n",
    "\n",
    "if not os.path.exists(token_dir):\n",
    "  os.makedirs(token_dir)\n",
    "\n",
    "tokenizer.save_model('wolfBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9hQqVS_qZWg",
    "outputId": "0cfc92c7-10d2-4dec-93e1-2a6397dc8d33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int', 'Ġmain', '(', 'int', 'Ġargc', ',', 'Ġvoid', 'Ġ**', 'argv', ')']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we can test the tokenizer\n",
    "tokenizer.encode(\"int main(int argc, void **argv)\").tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3hlRcyWsQjVq"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "Now, we can start preparing to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hO5M3vrAhcuj"
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# let's make sure that the tokenizer does not provide more tokens than we expect\n",
    "# we expect 510 tokens, because we will use the BERT model\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LTXXutqeDzPi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/miros/Documents/venv/book_ml/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import the RoBERTa configuration\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# initialize the configuration\n",
    "# please note that the vocab size is the same as the one in the tokenizer. \n",
    "# if it is not, we could get exceptions that the model and the tokenizer are not compatible\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-UsuK9Ps0H7",
    "outputId": "ec97bd11-0562-44f6-9663-aa7ae5fd4ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's print the configuration\n",
    "# please note that there is more parameters than what we configured\n",
    "# this is because we use the default values for the rest of the parameters\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzMqR-dzF4Ro",
    "outputId": "b43302a1-723d-4126-eb82-3bcb2f3fc584",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83504416\n",
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initializing a Model From Scratch\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# initialize the model\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "# let's print the number of parameters in the model\n",
    "print(model.num_parameters())\n",
    "\n",
    "# let's print the model\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset for training\n",
    "\n",
    "We use the datasets library from Hugging Face in order  to load the dataset. It allows us to work with larger datasets and in a more efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but before we actually train the model\n",
    "# we need to change the tokenizer to the one that we trained\n",
    "# and to make it compatible with the tokenizer that is expected by the model\n",
    "# so we read it from the file under a different tokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# initialize the tokenizer from the file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./wolfBERTa\", max_length=512)\n",
    "\n",
    "# please note that if we use a tokenizer that was trained before\n",
    "# the vanilla version of BPETokenizer, we will get an exception\n",
    "# that the BPE tokenizer is not collable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/miroslaw/.cache/huggingface/datasets/text/default-f26d7da2b304e662/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can change this to use the Dataset library instead of the transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "new_dataset = load_dataset(\"text\", data_files='./source_code_wolf_ssl.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miroslaw/.cache/huggingface/datasets/text/default-f26d7da2b304e662/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c73cfadced8bc12d_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "# now, let's tokenize the dataset\n",
    "\n",
    "# num_proc is the argument to use all cores\n",
    "tokenized_dataset = new_dataset.map(lambda x: tokenizer(x[\"text\"]), num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zTgWPa9Dipk2"
   },
   "outputs": [],
   "source": [
    "# training of the model requires a data collator\n",
    "# which creates a random set of tokens to mask\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YpvnFFmZJD-N"
   },
   "outputs": [],
   "source": [
    "# now, we can train the model\n",
    "# by creating the trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wolfBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=1_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if CUDA is available on this computer\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "VmaHZXzmkNtJ",
    "outputId": "000eb0e7-ce3a-460b-b972-3cea463bc2fe",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/miros/Documents/venv/book_ml/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1001' max='353681' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1001/353681 01:41 < 9:58:35, 9.82 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.229700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is where we train the model\n",
    "# which corresponds to the model.fit() method in Keras\n",
    "# which we used in the previous chapters\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-2hD_OLSa81"
   },
   "source": [
    "# Save the final model to hard drive\n",
    "\n",
    "Finally, we save the model to the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDNgPls7_l13",
    "outputId": "b86b8d70-2178-4a2e-bf09-ab417925698e"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./wolfBERTa\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The code above trained a model from scratch. The model is saved to the hard drive and can be used for further analysis.\n",
    "\n",
    "In chapter 9, we learned how to use such a model, so please take a look at that code in order to see how to use this model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
