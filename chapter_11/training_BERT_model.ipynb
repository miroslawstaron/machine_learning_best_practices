{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# Training a BERT model from scratch\n",
    "\n",
    "This demo illustrates the training of a BERT model from scratch. The model is trained on a small dataset of wolfSSL, which is an SSL library used in embedded software development. \n",
    "\n",
    "The notebook is based on the existing tutorial from Hugging Face [link](https://huggingface.co/blog/how-to-train).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aleXCJqVQPro"
   },
   "source": [
    "# Training a tokenizer\n",
    "\n",
    "The first step in training the model is to train the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMnymRDLe0hi",
    "outputId": "b7780f27-ce3b-4a3f-fcf5-ba11529b1d44",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files\n",
      "First file: source_code_wolf_ssl.txt\n"
     ]
    }
   ],
   "source": [
    "# We use the standard BPE tokenizer for this workbook\n",
    "# it was described in the previous chapter of the book\n",
    "# when we discussed feature extraction\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = ['source_code_wolf_ssl.txt']\n",
    "\n",
    "print(f'Found {len(paths)} files')\n",
    "print(f'First file: {paths[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "print('Training tokenizer...')\n",
    "\n",
    "# Customize training\n",
    "# we use a large vocabulary size, but we could also do with ca. 10_000\n",
    "tokenizer.train(files=paths, \n",
    "                vocab_size=52_000, \n",
    "                min_frequency=2, \n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqYKX1XYyRI-",
    "outputId": "8e2f0316-455d-443b-ca8b-146bdd2e3a6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolfBERTa/vocab.json', 'wolfBERTa/merges.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# we give this model a catchy name - wolfBERTa\n",
    "# because it is a RoBERTa model trained on the WolfSSL source code\n",
    "token_dir = './wolfBERTa'\n",
    "\n",
    "if not os.path.exists(token_dir):\n",
    "  os.makedirs(token_dir)\n",
    "\n",
    "tokenizer.save_model('wolfBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9hQqVS_qZWg",
    "outputId": "0cfc92c7-10d2-4dec-93e1-2a6397dc8d33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int', 'Ġmain', '(', 'int', 'Ġargc', ',', 'Ġvoid', 'Ġ**', 'argv', ')']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we can test the tokenizer\n",
    "tokenizer.encode(\"int main(int argc, void **argv)\").tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3hlRcyWsQjVq"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "Now, we can start preparing to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hO5M3vrAhcuj"
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# let's make sure that the tokenizer does not provide more tokens than we expect\n",
    "# we expect 510 tokens, because we will use the BERT model\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LTXXutqeDzPi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/miros/venv/book_ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import the RoBERTa configuration\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# initialize the configuration\n",
    "# please note that the vocab size is the same as the one in the tokenizer. \n",
    "# if it is not, we could get exceptions that the model and the tokenizer are not compatible\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-UsuK9Ps0H7",
    "outputId": "ec97bd11-0562-44f6-9663-aa7ae5fd4ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's print the configuration\n",
    "# please note that there is more parameters than what we configured\n",
    "# this is because we use the default values for the rest of the parameters\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzMqR-dzF4Ro",
    "outputId": "b43302a1-723d-4126-eb82-3bcb2f3fc584",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83504416\n",
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initializing a Model From Scratch\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# initialize the model\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "# let's print the number of parameters in the model\n",
    "print(model.num_parameters())\n",
    "\n",
    "# let's print the model\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset for training\n",
    "\n",
    "We use the datasets library from Hugging Face in order  to load the dataset. It allows us to work with larger datasets and in a more efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but before we actually train the model\n",
    "# we need to change the tokenizer to the one that we trained\n",
    "# and to make it compatible with the tokenizer that is expected by the model\n",
    "# so we read it from the file under a different tokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# initialize the tokenizer from the file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./wolfBERTa\", max_length=512)\n",
    "\n",
    "# please note that if we use a tokenizer that was trained before\n",
    "# the vanilla version of BPETokenizer, we will get an exception\n",
    "# that the BPE tokenizer is not collable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/miroslaw/.cache/huggingface/datasets/text/default-bb9f0226a4741661/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can change this to use the Dataset library instead of the transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "new_dataset = load_dataset(\"text\", data_files='./source_code_wolf_ssl.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/miroslaw/.cache/huggingface/datasets/text/default-bb9f0226a4741661/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c2662a868155bd2a_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "# now, let's tokenize the dataset\n",
    "\n",
    "# num_proc is the argument to use all cores\n",
    "tokenized_dataset = new_dataset.map(lambda x: tokenizer(x[\"text\"]), num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zTgWPa9Dipk2"
   },
   "outputs": [],
   "source": [
    "# training of the model requires a data collator\n",
    "# which creates a random set of tokens to mask\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YpvnFFmZJD-N"
   },
   "outputs": [],
   "source": [
    "# now, we can train the model\n",
    "# by creating the trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wolfBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if CUDA is available on this computer\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "VmaHZXzmkNtJ",
    "outputId": "000eb0e7-ce3a-460b-b972-3cea463bc2fe",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/miros/venv/book_ml/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='182312' max='353682' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [182312/353682 1:48:20 < 1:41:50, 28.05 it/s, Epoch 1.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.357800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.790700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.966200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.829900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.740100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.732000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.486400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.481900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.555600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.490400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.505700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.467500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.347800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.398600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>2.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>2.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>2.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>2.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>2.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>2.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>2.289400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>2.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>2.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>2.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>2.200800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>2.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>2.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>2.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>2.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>2.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>2.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>2.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>2.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>2.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>2.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>2.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>2.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>2.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>2.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>2.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>2.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>2.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>2.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>2.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>2.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>2.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>2.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>2.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>2.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>2.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>2.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>2.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>2.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>2.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>2.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>2.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>2.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>2.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>2.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>2.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>2.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>2.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>2.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>2.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>1.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>2.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>1.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>2.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>2.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>2.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>1.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>2.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>2.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>1.975900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>2.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>2.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>2.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>1.978600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>2.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>1.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>2.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>2.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.952600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>1.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>2.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>2.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>2.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>2.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>2.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>2.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>1.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>2.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>1.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>2.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>1.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>2.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>2.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>2.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>2.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>1.987300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>2.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>2.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>2.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>1.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>2.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>2.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>2.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>1.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>2.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>1.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>2.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>1.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>2.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>1.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>2.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>2.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>2.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>1.952800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>1.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>2.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>1.951100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>2.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>1.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>1.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>1.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>1.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>2.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>2.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>2.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>2.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>1.967300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>1.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>1.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>1.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>1.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>2.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>1.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>1.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>2.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>1.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>1.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>1.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>1.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>1.933600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>1.968300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>1.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>1.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>1.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>1.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>1.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>1.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>2.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>1.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>1.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>1.915800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>1.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>1.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>2.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>1.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>1.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>1.936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>1.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>1.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>1.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>1.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>1.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>1.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>1.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>1.903700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>1.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>2.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>1.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>1.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>1.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>1.931200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>1.953100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>1.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>1.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>1.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>1.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>1.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>1.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>1.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>1.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>1.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>1.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>1.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>1.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>1.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>1.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>1.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>1.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>1.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>1.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>1.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>1.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>1.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>1.836900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>1.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>1.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>1.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>1.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>1.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>1.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>1.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>1.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>1.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>1.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>1.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>1.797600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>1.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>1.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>1.896500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>1.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>1.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>1.839600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>1.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>1.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>1.947800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>1.849900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>1.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>1.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>1.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>1.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146500</td>\n",
       "      <td>1.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>1.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>1.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>1.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148500</td>\n",
       "      <td>1.921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>1.809900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149500</td>\n",
       "      <td>1.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>1.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150500</td>\n",
       "      <td>1.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>1.883900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151500</td>\n",
       "      <td>1.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>1.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>1.838400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>1.822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153500</td>\n",
       "      <td>1.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>1.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154500</td>\n",
       "      <td>1.836100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>1.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155500</td>\n",
       "      <td>1.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>1.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156500</td>\n",
       "      <td>1.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>1.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>1.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>1.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158500</td>\n",
       "      <td>1.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>1.840800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159500</td>\n",
       "      <td>1.842500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>1.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160500</td>\n",
       "      <td>1.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>1.851600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161500</td>\n",
       "      <td>1.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>1.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>1.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>1.855400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163500</td>\n",
       "      <td>1.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>1.830500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164500</td>\n",
       "      <td>1.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>1.833500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165500</td>\n",
       "      <td>1.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>1.850300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166500</td>\n",
       "      <td>1.901600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>1.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>1.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>1.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168500</td>\n",
       "      <td>1.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>1.821800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169500</td>\n",
       "      <td>1.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>1.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170500</td>\n",
       "      <td>1.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>1.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171500</td>\n",
       "      <td>1.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>1.844300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>1.831800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>1.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173500</td>\n",
       "      <td>1.801300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>1.793500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174500</td>\n",
       "      <td>1.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>1.819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175500</td>\n",
       "      <td>1.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>1.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176500</td>\n",
       "      <td>1.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>1.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177500</td>\n",
       "      <td>1.804700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>1.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178500</td>\n",
       "      <td>1.833400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179000</td>\n",
       "      <td>1.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179500</td>\n",
       "      <td>1.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>1.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180500</td>\n",
       "      <td>1.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181000</td>\n",
       "      <td>1.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181500</td>\n",
       "      <td>1.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182000</td>\n",
       "      <td>1.825400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is where we train the model\n",
    "# which corresponds to the model.fit() method in Keras\n",
    "# which we used in the previous chapters\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-2hD_OLSa81"
   },
   "source": [
    "# Save the final model to hard drive\n",
    "\n",
    "Finally, we save the model to the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDNgPls7_l13",
    "outputId": "b86b8d70-2178-4a2e-bf09-ab417925698e"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./wolfBERTa\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The code above trained a model from scratch. The model is saved to the hard drive and can be used for further analysis.\n",
    "\n",
    "In chapter 9, we learned how to use such a model, so please take a look at that code in order to see how to use this model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
